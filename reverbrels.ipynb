{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reverbrels.ipynb",
      "provenance": [],
      "mount_file_id": "1yF8JgEhxnwRMJmoz-jk0fP-oC0lUSdKb",
      "authorship_tag": "ABX9TyP8RaKsW4wiNF1gmgmYXGLq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meti-94/BERT-QA/blob/main/reverbrels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBeeHMUcQqS9",
        "outputId": "8c32833a-b991-4b6d-81f3-17b60bd4ef20"
      },
      "source": [
        "!pip install fuzzywuzzy -q\n",
        "!pip install pattern -q"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 22.2 MB 61.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 46.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 41.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 419 kB 45.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 38.5 MB/s \n",
            "\u001b[?25h  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51wZ5qzvU_lB"
      },
      "source": [
        "def get_tf_idf_query_similarity(vectorizer, docs_tfidf, query):\n",
        "    \"\"\"\n",
        "    vectorizer: TfIdfVectorizer model\n",
        "    docs_tfidf: tfidf vectors for all docs\n",
        "    query: query doc\n",
        "    return: cosine similarity between query and all docs\n",
        "    \"\"\"\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    cosineSimilarities = cosine_similarity(query_tfidf, docs_tfidf).flatten()\n",
        "    return cosineSimilarities"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7PXi2b4Qir4",
        "outputId": "c32237f9-9b18-4203-ad7f-488a5149eefb"
      },
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG,PAST\n",
        "import sys\n",
        "\n",
        "\n",
        "#### pattern python>=3.7 compatibility problem\n",
        "def pattern_stopiteration_workaround():\n",
        "    try:\n",
        "        print(lexeme('gave'))\n",
        "    except:\n",
        "        pass\n",
        "pattern_stopiteration_workaround()\n",
        "\n",
        "\n",
        "class ReverbKnowledgeBase:\n",
        "\tdef __init__(self, path='../data/reverb_wikipedia_tuples-1.1.txt'):\n",
        "\t\tsuper().__init__()\n",
        "\t\tdf = pd.read_csv(path, sep='\\t', header=None)\n",
        "\t\treverb_columns_name = ['ExID', 'arg1', 'rel', 'arg2', 'narg1', 'nrel', 'narg2', 'csents', 'conf', 'urls']\n",
        "\t\tdf.columns = reverb_columns_name\n",
        "\t\tdf = df.dropna()\n",
        "\t\tdf = df.drop_duplicates()\n",
        "\t\tself.KB = df\n",
        "\t\tself.is_facts = self.KB[(self.KB.rel.apply(lambda rg:rg.find('is ')!=-1))|(self.KB.rel.apply(lambda rg:rg.find('Is ')!=-1))]\n",
        "\t\tself.nodes = self.KB['arg1'].to_list()+self.KB['arg2'].to_list()\n",
        "\t\tself.edges = self.KB['rel'].to_list()\n",
        "\t\tself.nodes_vectorizer = TfidfVectorizer()\n",
        "\t\tself.edges_vectorizer = TfidfVectorizer()\n",
        "\t\tself.nodes_tfidf = self.nodes_vectorizer.fit_transform(self.nodes)\n",
        "\t\tself.edges_tfidf = self.edges_vectorizer.fit_transform(self.edges)\n",
        "\t\tself.relations = {}\n",
        "\t\tfor index, row in tqdm(df.iterrows(), total=df.shape[0], desc='Indexing ...'):\n",
        "\t\t\tif row['rel'] in self.relations:\n",
        "\t\t\t\tself.relations[row['rel']].append((row['arg1'], index, row['conf']))\n",
        "\t\t\t\tself.relations[row['rel']].append((row['arg2'], index, row['conf']))\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.relations[row['rel']] = [(row['arg1'], index, row['conf'])]\n",
        "\t\t\t\tself.relations[row['rel']].append((row['arg2'], index, row['conf']))\n",
        "\t\t\n",
        "\n",
        "\n",
        "\tdef tfidf_nodes_query(self, search_phrase, cutoff=50):\n",
        "\t\tsimilarities = get_tf_idf_query_similarity(self.nodes_vectorizer, self.nodes_tfidf, search_phrase)\n",
        "\t\tranks = {k:v for k,v in zip(self.nodes, similarities)}\n",
        "\t\tsorted_ranks = {k: v for k, v in sorted(ranks.items(), key=lambda item:item[1], reverse=True)[:min(len(ranks), cutoff)]}\n",
        "\n",
        "\t\treturn sorted_ranks\n",
        "\n",
        "\tdef tfidf_edges_query(self, search_phrase, cutoff=50):\n",
        "\t\tsimilarities = get_tf_idf_query_similarity(self.edges_vectorizer, self.edges_tfidf, search_phrase)\n",
        "\t\tranks = {k:v for k,v in zip(self.edges, similarities)}\n",
        "\t\tsorted_ranks = {k: v for k, v in sorted(ranks.items(), key=lambda item:item[1], reverse=True)[:min(len(ranks), cutoff)]}\n",
        "\t\treturn sorted_ranks\n",
        "\n",
        "\tdef tfidf_query(self, node='Bill Gates', edge='Born'):\n",
        "\t\t# print(edge)\n",
        "\t\tedge_list = edge.split()\n",
        "\t\tif len(edge_list)>=2 and edge_list[0]=='did':\n",
        "\t\t\tedge_list[1] = conjugate(verb=edge_list[1],tense=PAST)\n",
        "\t\t\tedge = ' '.join(edge_list[1:])\n",
        "\t\telse:\n",
        "\t\t\tedge = ' '.join(edge_list)\n",
        "\t\t# print(edge)\n",
        "\t\tif edge.strip()!='is':\n",
        "\t\t\tnodes = self.tfidf_nodes_query(node)\n",
        "\t\t\tedges = self.tfidf_edges_query(edge)\n",
        "\t\t\tpruned = []\n",
        "\t\t\tfor node in nodes.keys():\n",
        "\t\t\t\tfor edge in edges.keys():\n",
        "\t\t\t\t\tfor item in self.relations[edge]:\n",
        "\t\t\t\t\t\tif item[0]==node:\n",
        "\t\t\t\t\t\t\tpruned.append((item[1], item[-1], nodes[node], edges[edge]))\n",
        "\t\t\tsorted_pruned = sorted(pruned, key=lambda x:x[2]+x[3], reverse=True)\n",
        "\t\t\treturn sorted_pruned[:min(len(sorted_pruned), 100)]\n",
        "\t\telse:\n",
        "\t\t\tnodes = self.tfidf_nodes_query(node)\n",
        "\t\t\targ1 = self.KB.loc[self.KB['arg1'].isin(nodes.keys())]\n",
        "\t\t\targ2 = self.KB.loc[self.KB['arg2'].isin(nodes.keys())]\n",
        "\t\t\t# print(self.KB.loc[self.KB['arg2'].isin(nodes.keys())][:10])\n",
        "\t\t\t\n",
        "\t\t\tpruned = []\n",
        "\t\t\tfor node, similarity in nodes.items():\n",
        "\t\t\t\tfor idx, row in arg1.loc[arg1['arg1']==node].iterrows():\n",
        "\t\t\t\t\ttemp1 = self.edges_vectorizer.transform([row['rel']])\n",
        "\t\t\t\t\ttemp2 = self.edges_vectorizer.transform([edge])\n",
        "\t\t\t\t\tedge_similarity = cosine_similarity(temp1, temp2).flatten().item()\n",
        "\t\t\t\t\tpruned.append((idx, row['conf'], similarity, edge_similarity))\n",
        "\t\t\t\tfor idx, row in arg2.loc[arg2['arg2']==node].iterrows():\n",
        "\t\t\t\t\ttemp1 = self.edges_vectorizer.transform([row['rel']])\n",
        "\t\t\t\t\ttemp2 = self.edges_vectorizer.transform([edge])\n",
        "\t\t\t\t\tedge_similarity = cosine_similarity(temp1, temp2).flatten().item()\n",
        "\t\t\t\t\tpruned.append((idx, row['conf'], similarity, edge_similarity))\n",
        "\t\t\tsorted_pruned = sorted(pruned, key=lambda x:x[2]+x[3], reverse=True)\n",
        "\t\t\treturn sorted_pruned[:min(len(sorted_pruned), 100)]\n",
        "\n",
        "if __name__=='__main__':\n",
        "\tRKBG = ReverbKnowledgeBase(r'/content/drive/MyDrive/data_freebase/reverb_wikipedia_tuples-1.1.txt') #\t'./sample_reverb_tuples.txt'\n",
        "\t# print(len(RKBG.nodes_vectorizer.vocabulary_), len(RKBG.edges_vectorizer.vocabulary_))\n",
        "\t# print(RKBG.tfidf_query(node='fishkind', edge='grew up in'))\n",
        "\tprint(RKBG.tfidf_query(node='abegg', edge='did die'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['give', 'gives', 'giving', 'gave', 'given']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Indexing ...: 100%|██████████| 407236/407236 [00:46<00:00, 8720.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(17829, 0.95825, 1.0, 0.9271252473625466)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCCbaIlHQlIE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_eval.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meti-94/BERT-QA/blob/main/train_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ-LV62ByxcR",
        "outputId": "7929975b-57c3-451f-91a8-cc0a5b667b32"
      },
      "source": [
        "!pip install transformers -q\n",
        "!pip uninstall -y tensorflow -q\n",
        "!pip install fuzzywuzzy[speedup] -q\n",
        "!pip install networkx -q\n",
        "!pip install pytorch-crf -q\n",
        "!pip install pattern -q\n",
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.8 MB 20.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 57.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 54.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 174 kB 29.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 58.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 22.2 MB 1.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 88 kB 8.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 81 kB 11.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 68.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 62.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 348 kB 75.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 100 kB 12.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 61.3 MB/s \n",
            "\u001b[?25h  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK5AY1cWmoqG",
        "outputId": "229b7691-1d0a-4f12-9bcc-cdaaccdeb984"
      },
      "source": [
        "!git clone https://github.com/meti-94/BERT-QA.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BERT-QA'...\n",
            "remote: Enumerating objects: 586, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 586 (delta 21), reused 47 (delta 16), pack-reused 531\u001b[K\n",
            "Receiving objects: 100% (586/586), 114.72 MiB | 17.59 MiB/s, done.\n",
            "Resolving deltas: 100% (256/256), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-eIIezrzF9g",
        "outputId": "8fce68f1-da2a-4197-d0c7-1ac62144cd13"
      },
      "source": [
        "%cd /content/BERT-QA/src\n",
        "# model types : [MultiDepthNodeEdgeDetector, BertLSTMCRF, BertCNN, NodeEdgeDetector]\n",
        "# cross validation: [True, False]\n",
        "!python train.py NodeEdgeDetector True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BERT-QA/src\n",
            "['train.py', 'NodeEdgeDetector', 'True']\n",
            "INFO:root:\n",
            "\n",
            "############# Fold Number 1 #############\n",
            "\n",
            "\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "INFO:root:Train Dataset Contains 10630 Samples.\n",
            "INFO:root:Valid Dataset Contains 1876 Samples.\n",
            "INFO:root:Test Dataset Contains 4169 Samples.\n",
            "Train Epoch Number 1: 100% 54/54 [00:28<00:00,  1.86it/s]\n",
            "INFO:root:Epoch number: 1 Train Loss is equal: 528.4711303710938\n",
            "Eval Epoch Number 1: 100% 10/10 [00:01<00:00,  6.52it/s]\n",
            "INFO:root:Epoch number: 1 Eval Loss is equal: 31.972412109375\n",
            "Train Epoch Number 2: 100% 54/54 [00:28<00:00,  1.89it/s]\n",
            "INFO:root:Epoch number: 2 Train Loss is equal: 32.30843734741211\n",
            "Eval Epoch Number 2: 100% 10/10 [00:01<00:00,  6.62it/s]\n",
            "INFO:root:Epoch number: 2 Eval Loss is equal: 16.474538803100586\n",
            "Train Epoch Number 3: 100% 54/54 [00:27<00:00,  1.94it/s]\n",
            "INFO:root:Epoch number: 3 Train Loss is equal: 20.844181060791016\n",
            "Eval Epoch Number 3: 100% 10/10 [00:01<00:00,  6.72it/s]\n",
            "INFO:root:Epoch number: 3 Eval Loss is equal: 13.469278335571289\n",
            "Train Epoch Number 4: 100% 54/54 [00:28<00:00,  1.93it/s]\n",
            "INFO:root:Epoch number: 4 Train Loss is equal: 16.410255432128906\n",
            "Eval Epoch Number 4: 100% 10/10 [00:01<00:00,  6.61it/s]\n",
            "INFO:root:Epoch number: 4 Eval Loss is equal: 12.73450756072998\n",
            "Train Epoch Number 5: 100% 54/54 [00:28<00:00,  1.92it/s]\n",
            "INFO:root:Epoch number: 5 Train Loss is equal: 13.00365924835205\n",
            "Eval Epoch Number 5: 100% 10/10 [00:01<00:00,  6.63it/s]\n",
            "INFO:root:Epoch number: 5 Eval Loss is equal: 11.409743309020996\n",
            "Train Epoch Number 6: 100% 54/54 [00:27<00:00,  1.93it/s]\n",
            "INFO:root:Epoch number: 6 Train Loss is equal: 10.567378044128418\n",
            "Eval Epoch Number 6: 100% 10/10 [00:01<00:00,  6.62it/s]\n",
            "INFO:root:Epoch number: 6 Eval Loss is equal: 10.791011810302734\n",
            "Predicting ...: 100% 42/42 [00:03<00:00, 12.21it/s]\n",
            "INFO:root:Dataset-wide F1, precision and recall:\n",
            "INFO:root:0.9907025884838879, 0.9915406577138628, 0.9898659347619551\n",
            "INFO:root:Averaged F1, precision and recall:\n",
            "INFO:root:0.9891770088848592, 0.988738306548333, 0.9896161006978949\n",
            "INFO:root:Span accuracy\n",
            "INFO:root:0.9863276565123531\n",
            "INFO:root:Dataset-wide F1, precision and recall:\n",
            "INFO:root:0.9925490520738472, 0.9926312303361484, 0.9924668874172186\n",
            "INFO:root:Averaged F1, precision and recall:\n",
            "INFO:root:0.995161946574353, 0.9951906932118015, 0.9951332015975817\n",
            "INFO:root:Span accuracy\n",
            "INFO:root:0.9716958503238187\n",
            "Question                    Node               Edge\n",
            "--------------------------  -----------------  --------------------\n",
            "Where was Bill Gates Born?  ['bill', 'gates']  ['was', 'born', '?']\n",
            "INFO:root:\n",
            "\n",
            "############# Fold Number 2 #############\n",
            "\n",
            "\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
            "INFO:root:Train Dataset Contains 10630 Samples.\n",
            "INFO:root:Valid Dataset Contains 1876 Samples.\n",
            "INFO:root:Test Dataset Contains 4169 Samples.\n",
            "Train Epoch Number 1: 100% 54/54 [00:27<00:00,  1.94it/s]\n",
            "INFO:root:Epoch number: 1 Train Loss is equal: 405.2223815917969\n",
            "Eval Epoch Number 1: 100% 10/10 [00:01<00:00,  6.54it/s]\n",
            "INFO:root:Epoch number: 1 Eval Loss is equal: 29.6771297454834\n",
            "Train Epoch Number 2: 100% 54/54 [00:27<00:00,  1.94it/s]\n",
            "INFO:root:Epoch number: 2 Train Loss is equal: 28.583383560180664\n",
            "Eval Epoch Number 2: 100% 10/10 [00:01<00:00,  6.59it/s]\n",
            "INFO:root:Epoch number: 2 Eval Loss is equal: 16.833349227905273\n",
            "Train Epoch Number 3: 100% 54/54 [00:27<00:00,  1.96it/s]\n",
            "INFO:root:Epoch number: 3 Train Loss is equal: 17.650339126586914\n",
            "Eval Epoch Number 3: 100% 10/10 [00:01<00:00,  6.62it/s]\n",
            "INFO:root:Epoch number: 3 Eval Loss is equal: 13.498250007629395\n",
            "Train Epoch Number 4: 100% 54/54 [00:27<00:00,  1.96it/s]\n",
            "INFO:root:Epoch number: 4 Train Loss is equal: 14.476200103759766\n",
            "Eval Epoch Number 4: 100% 10/10 [00:01<00:00,  6.61it/s]\n",
            "INFO:root:Epoch number: 4 Eval Loss is equal: 12.402645111083984\n",
            "Train Epoch Number 5: 100% 54/54 [00:27<00:00,  1.95it/s]\n",
            "INFO:root:Epoch number: 5 Train Loss is equal: 11.428360939025879\n",
            "Eval Epoch Number 5: 100% 10/10 [00:01<00:00,  6.54it/s]\n",
            "INFO:root:Epoch number: 5 Eval Loss is equal: 14.431727409362793\n",
            "Train Epoch Number 6: 100% 54/54 [00:27<00:00,  1.95it/s]\n",
            "INFO:root:Epoch number: 6 Train Loss is equal: 9.746760368347168\n",
            "Eval Epoch Number 6: 100% 10/10 [00:01<00:00,  6.55it/s]\n",
            "INFO:root:Epoch number: 6 Eval Loss is equal: 14.03947925567627\n",
            "Predicting ...: 100% 42/42 [00:03<00:00, 11.66it/s]\n",
            "INFO:root:Dataset-wide F1, precision and recall:\n",
            "INFO:root:0.9882604055496266, 0.9865757511186874, 0.9899508231772504\n",
            "INFO:root:Averaged F1, precision and recall:\n",
            "INFO:root:0.9888337254843277, 0.9882174553678511, 0.9894507647146169\n",
            "INFO:root:Span accuracy\n",
            "INFO:root:0.9846485967858\n",
            "INFO:root:Dataset-wide F1, precision and recall:\n",
            "INFO:root:0.9920956710488594, 0.9935198096956771, 0.9906756093571079\n",
            "INFO:root:Averaged F1, precision and recall:\n",
            "INFO:root:0.9956585023002773, 0.9960510685444721, 0.9952662453726373\n",
            "INFO:root:Span accuracy\n",
            "INFO:root:0.9772127608539218\n",
            "Question                    Node               Edge\n",
            "--------------------------  -----------------  --------------------\n",
            "Where was Bill Gates Born?  ['bill', 'gates']  ['was', 'born', '?']\n",
            "INFO:root:\n",
            "\n",
            "############# Fold Number 3 #############\n",
            "\n",
            "\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
            "INFO:root:Train Dataset Contains 10630 Samples.\n",
            "INFO:root:Valid Dataset Contains 1876 Samples.\n",
            "INFO:root:Test Dataset Contains 4169 Samples.\n",
            "Train Epoch Number 1:  59% 32/54 [00:17<00:11,  1.90it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6h_BRN-zs6k",
        "outputId": "53c6410c-0ee6-46b8-ecde-493557c85e1e"
      },
      "source": [
        "!python evaluation.py "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/added_tokens.json HTTP/1.1\" 404 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/special_tokens_map.json HTTP/1.1\" 404 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "Indexing ...: 100% 407236/407236 [00:22<00:00, 17735.20it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3361, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 76, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 108, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'Reverb_no'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"evaluation.py\", line 24, in <module>\n",
            "    actual = test_df['Reverb_no'].to_list()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\", line 3458, in __getitem__\n",
            "    indexer = self.columns.get_loc(key)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\", line 3363, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'Reverb_no'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOD_Mka8WaGy",
        "outputId": "4b3e8bf2-11c1-4eb0-fb82-3b07446dbe8f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh5jp7fcWawz"
      },
      "source": [
        "!cp /content/drive/MyDrive/reverb_wikipedia_tuples-1.1.txt /content/BERT-QA/data\n",
        "# !cp /content/drive/MyDrive/models/test.xlsx /content/BERT-QA/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt-eNG27YniQ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}
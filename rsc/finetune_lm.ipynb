{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune_lm.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNwGFKK43unnN1OBsEyNq7F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meti-94/BERT-QA/blob/main/finetune_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning BERT on **ReverbSimpleQuestions** and **SimpleQuestions** dataset"
      ],
      "metadata": {
        "id": "YRyaLAfbYxwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers -q\n",
        "!pip uninstall -y tensorflow -q\n",
        "!pip install datasets -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vwDn8wjYxCM",
        "outputId": "99d8d7fa-7221-45d7-a855-b499c07e29d0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 27.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 63.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.3 MB/s \n",
            "\u001b[?25h  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 346 kB 36.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 76.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 75.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 30.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 71.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 70.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 2.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 77.1 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, which is not installed.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nF5c2YrYWCM",
        "outputId": "0fe5c533-47eb-461f-c726-3bf6ae1e5bd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BERT-QA'...\n",
            "remote: Enumerating objects: 531, done.\u001b[K\n",
            "remote: Total 531 (delta 0), reused 0 (delta 0), pack-reused 531\u001b[K\n",
            "Receiving objects: 100% (531/531), 95.82 MiB | 15.28 MiB/s, done.\n",
            "Resolving deltas: 100% (235/235), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/meti-94/BERT-QA.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQErjWWvagD5",
        "outputId": "87c7a9c2-0bee-4713-bacb-99e49fa99e4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "SQ_train = pd.read_csv('/content/BERT-QA/BuboQA/Freebase/train.txt', sep='\\t').iloc[:, 5].to_list()\n",
        "SQ_valid = pd.read_csv('/content/BERT-QA/BuboQA/Freebase/valid.txt', sep='\\t').iloc[:, 5].to_list()\n",
        "RSQ_train = pd.read_excel('/content/drive/MyDrive/QA/train.xlsx')['Question'].to_list()\n",
        "RSQ_valid = pd.read_excel('/content/drive/MyDrive/QA/valid.xlsx')['Question'].to_list()\n",
        "train_data = SQ_train+RSQ_train\n",
        "valid_data = SQ_valid+RSQ_valid\n",
        "with open('sq_train.txt', 'w') as fout:\n",
        "  fout.write('\\n'.join(SQ_train))\n",
        "with open('sq_valid.txt', 'w') as fout:\n",
        "  fout.write('\\n'.join(SQ_valid))\n",
        "with open('rsq_train.txt', 'w') as fout:\n",
        "  fout.write('\\n'.join(RSQ_train))\n",
        "with open('rsq_valid.txt', 'w') as fout:\n",
        "  fout.write('\\n'.join(RSQ_valid))\n",
        "with open('train_data.txt', 'w') as fout:\n",
        "  fout.write('\\n'.join(train_data))\n",
        "with open('valid_data.txt', 'w') as fout:\n",
        "  fout.write('\\n'.join(valid_data))\n",
        "\n",
        "# print(len(SQ_train), len(SQ_valid), len(RSQ_train), len(RSQ_valid), len(train_data), len(valid_data))"
      ],
      "metadata": {
        "id": "Sfh_9sI6bo36"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbTgUx6RcPhh",
        "outputId": "cd537b98-9648-4dbc-fc9a-e0728589d4a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 97916, done.\u001b[K\n",
            "remote: Counting objects: 100% (467/467), done.\u001b[K\n",
            "remote: Compressing objects: 100% (262/262), done.\u001b[K\n",
            "remote: Total 97916 (delta 221), reused 366 (delta 180), pack-reused 97449\u001b[K\n",
            "Receiving objects: 100% (97916/97916), 91.47 MiB | 17.68 MiB/s, done.\n",
            "Resolving deltas: 100% (71930/71930), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/transformers/examples/pytorch/language-modeling/run_mlm.py \\\n",
        "  --model_name_or_path bert-base-uncased \\\n",
        "  --train_file /content/sq_train.txt \\\n",
        "  --validation_file /content/sq_valid.txt \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --per_device_eval_batch_size 16 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --output_dir ./finetune-mlm \\\n",
        "  --line_by_line \\\n",
        "  --num_train_epochs 5 \\\n",
        "  --save_total_limit 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J2Ir21cjJwx",
        "outputId": "9d9d5dcd-0f20-4b91-da42-dd620dfe5df3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06/10/2022 09:45:24 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "06/10/2022 09:45:24 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./finetune-mlm/runs/Jun10_09-45-23_4c73e28cb7f7,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=./finetune-mlm,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./finetune-mlm,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "06/10/2022 09:45:25 - WARNING - datasets.builder - Using custom data configuration default-67ef324d0185fdbe\n",
            "06/10/2022 09:45:25 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-67ef324d0185fdbe/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
            "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-67ef324d0185fdbe/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 10305.42it/s]\n",
            "06/10/2022 09:45:25 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "06/10/2022 09:45:25 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1319.17it/s]\n",
            "06/10/2022 09:45:25 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "06/10/2022 09:45:25 - INFO - datasets.builder - Generating train split\n",
            "06/10/2022 09:45:25 - INFO - datasets.builder - Generating validation split\n",
            "06/10/2022 09:45:25 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-67ef324d0185fdbe/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 928.77it/s]\n",
            "[INFO|hub.py:584] 2022-06-10 09:45:26,004 >> https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbkau1q4n\n",
            "Downloading: 100% 570/570 [00:00<00:00, 522kB/s]\n",
            "[INFO|hub.py:588] 2022-06-10 09:45:26,884 >> storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|hub.py:596] 2022-06-10 09:45:26,885 >> creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:659] 2022-06-10 09:45:26,885 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:708] 2022-06-10 09:45:26,886 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:584] 2022-06-10 09:45:27,771 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8w1vxexr\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 24.9kB/s]\n",
            "[INFO|hub.py:588] 2022-06-10 09:45:28,665 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|hub.py:596] 2022-06-10 09:45:28,665 >> creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:659] 2022-06-10 09:45:29,544 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:708] 2022-06-10 09:45:29,544 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:584] 2022-06-10 09:45:31,323 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphtr2vf0s\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 269kB/s]\n",
            "[INFO|hub.py:588] 2022-06-10 09:45:33,069 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|hub.py:596] 2022-06-10 09:45:33,069 >> creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|hub.py:584] 2022-06-10 09:45:33,945 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpp9e_0u3q\n",
            "Downloading: 100% 455k/455k [00:01<00:00, 433kB/s]\n",
            "[INFO|hub.py:588] 2022-06-10 09:45:35,909 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|hub.py:596] 2022-06-10 09:45:35,909 >> creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-06-10 09:45:38,573 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-06-10 09:45:38,573 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-06-10 09:45:38,573 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-06-10 09:45:38,573 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-06-10 09:45:38,573 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:659] 2022-06-10 09:45:39,461 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:708] 2022-06-10 09:45:39,462 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:584] 2022-06-10 09:45:40,374 >> https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvkyxko8n\n",
            "Downloading: 100% 420M/420M [00:06<00:00, 65.2MB/s]\n",
            "[INFO|hub.py:588] 2022-06-10 09:45:47,160 >> storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[INFO|hub.py:596] 2022-06-10 09:45:47,160 >> creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[INFO|modeling_utils.py:2104] 2022-06-10 09:45:47,160 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:2471] 2022-06-10 09:45:48,777 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:2489] 2022-06-10 09:45:48,777 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
            "Running tokenizer on dataset line_by_line:   0% 0/76 [00:00<?, ?ba/s]06/10/2022 09:45:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-67ef324d0185fdbe/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-d28cb87f298fa8fe.arrow\n",
            "Running tokenizer on dataset line_by_line: 100% 76/76 [00:03<00:00, 23.64ba/s]\n",
            "Running tokenizer on dataset line_by_line:   0% 0/11 [00:00<?, ?ba/s]06/10/2022 09:45:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-67ef324d0185fdbe/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-760fd21f49327585.arrow\n",
            "Running tokenizer on dataset line_by_line: 100% 11/11 [00:00<00:00, 25.10ba/s]\n",
            "06/10/2022 09:45:53 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/accuracy/accuracy.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp4n4y6bg2\n",
            "Downloading builder script: 4.21kB [00:00, 4.11MB/s]       \n",
            "06/10/2022 09:45:53 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/accuracy/accuracy.py in cache at /root/.cache/huggingface/datasets/downloads/302a01b7f8daba5e0dc4e13a39ce01c6bc75cb5206e425cb0136bceff99dda46.32b3507481ea2e26fd6a2b34c9976e9da377302faaf35089eb1cd971d41bb0ff.py\n",
            "06/10/2022 09:45:53 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/302a01b7f8daba5e0dc4e13a39ce01c6bc75cb5206e425cb0136bceff99dda46.32b3507481ea2e26fd6a2b34c9976e9da377302faaf35089eb1cd971d41bb0ff.py\n",
            "[INFO|trainer.py:649] 2022-06-10 09:46:04,813 >> The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1474] 2022-06-10 09:46:04,825 >> ***** Running training *****\n",
            "[INFO|trainer.py:1475] 2022-06-10 09:46:04,825 >>   Num examples = 75721\n",
            "[INFO|trainer.py:1476] 2022-06-10 09:46:04,825 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1477] 2022-06-10 09:46:04,826 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1478] 2022-06-10 09:46:04,826 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1479] 2022-06-10 09:46:04,826 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1480] 2022-06-10 09:46:04,826 >>   Total optimization steps = 23665\n",
            "{'loss': 2.8631, 'learning_rate': 4.8943587576589904e-05, 'epoch': 0.11}\n",
            "  2% 500/23665 [00:49<39:00,  9.90it/s][INFO|trainer.py:2458] 2022-06-10 09:46:54,594 >> Saving model checkpoint to ./finetune-mlm/checkpoint-500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 09:46:54,594 >> Configuration saved in ./finetune-mlm/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 09:46:55,839 >> Model weights saved in ./finetune-mlm/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 09:46:55,840 >> tokenizer config file saved in ./finetune-mlm/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 09:46:55,840 >> Special tokens file saved in ./finetune-mlm/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 2.6017, 'learning_rate': 4.7887175153179806e-05, 'epoch': 0.21}\n",
            "  4% 1000/23665 [01:45<38:18,  9.86it/s][INFO|trainer.py:2458] 2022-06-10 09:47:50,589 >> Saving model checkpoint to ./finetune-mlm/checkpoint-1000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 09:47:50,589 >> Configuration saved in ./finetune-mlm/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 09:47:51,649 >> Model weights saved in ./finetune-mlm/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 09:47:51,649 >> tokenizer config file saved in ./finetune-mlm/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 09:47:51,649 >> Special tokens file saved in ./finetune-mlm/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 2.5204, 'learning_rate': 4.683076272976971e-05, 'epoch': 0.32}\n",
            "  6% 1500/23665 [02:41<39:23,  9.38it/s][INFO|trainer.py:2458] 2022-06-10 09:48:46,781 >> Saving model checkpoint to ./finetune-mlm/checkpoint-1500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 09:48:46,782 >> Configuration saved in ./finetune-mlm/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 09:48:47,942 >> Model weights saved in ./finetune-mlm/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 09:48:47,942 >> tokenizer config file saved in ./finetune-mlm/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 09:48:47,942 >> Special tokens file saved in ./finetune-mlm/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 09:48:51,451 >> Deleting older checkpoint [finetune-mlm/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 2.5652, 'learning_rate': 4.57743503063596e-05, 'epoch': 0.42}\n",
            "  8% 2000/23665 [03:38<36:51,  9.80it/s][INFO|trainer.py:2458] 2022-06-10 09:49:42,988 >> Saving model checkpoint to ./finetune-mlm/checkpoint-2000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 09:49:42,988 >> Configuration saved in ./finetune-mlm/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 09:49:44,043 >> Model weights saved in ./finetune-mlm/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 09:49:44,044 >> tokenizer config file saved in ./finetune-mlm/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 09:49:44,044 >> Special tokens file saved in ./finetune-mlm/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 09:49:47,652 >> Deleting older checkpoint [finetune-mlm/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 2.4959, 'learning_rate': 4.4717937882949505e-05, 'epoch': 0.53}\n",
            " 11% 2500/23665 [04:34<36:13,  9.74it/s][INFO|trainer.py:2458] 2022-06-10 09:50:39,109 >> Saving model checkpoint to ./finetune-mlm/checkpoint-2500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 09:50:39,110 >> Configuration saved in ./finetune-mlm/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 09:50:40,182 >> Model weights saved in ./finetune-mlm/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 09:50:40,182 >> tokenizer config file saved in ./finetune-mlm/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 09:50:40,182 >> Special tokens file saved in ./finetune-mlm/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 09:50:43,969 >> Deleting older checkpoint [finetune-mlm/checkpoint-1500] due to args.save_total_limit\n",
            "{'loss': 2.4767, 'learning_rate': 4.3661525459539406e-05, 'epoch': 0.63}\n",
            " 13% 3000/23665 [05:31<47:45,  7.21it/s][INFO|trainer.py:2458] 2022-06-10 09:51:36,811 >> Saving model checkpoint to ./finetune-mlm/checkpoint-3000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 09:51:36,812 >> Configuration saved in ./finetune-mlm/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 09:51:38,176 >> Model weights saved in ./finetune-mlm/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 09:51:38,177 >> tokenizer config file saved in ./finetune-mlm/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 09:51:38,177 >> Special tokens file saved in ./finetune-mlm/checkpoint-3000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 09:51:41,722 >> Deleting older checkpoint [finetune-mlm/checkpoint-2000] due to args.save_total_limit\n",
            "{'loss': 2.5048, 'learning_rate': 4.260511303612931e-05, 'epoch': 0.74}\n",
            " 15% 3500/23665 [06:30<33:31, 10.03it/s][INFO|trainer.py:2458] 2022-06-10 09:52:34,899 >> Saving model checkpoint to ./finetune-mlm/checkpoint-3500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 09:52:34,900 >> Configuration saved in ./finetune-mlm/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 09:52:36,128 >> Model weights saved in ./finetune-mlm/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 09:52:36,129 >> tokenizer config file saved in ./finetune-mlm/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 09:52:36,129 >> Special tokens file saved in ./finetune-mlm/checkpoint-3500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 09:52:39,637 >> Deleting older checkpoint [finetune-mlm/checkpoint-2500] due to args.save_total_limit\n",
            "{'loss': 2.3962, 'learning_rate': 4.154870061271921e-05, 'epoch': 0.85}\n",
            " 17% 4000/23665 [07:26<35:14,  9.30it/s][INFO|trainer.py:2458] 2022-06-10 09:53:31,171 >> Saving model checkpoint to ./finetune-mlm/checkpoint-4000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 09:53:31,172 >> Configuration saved in ./finetune-mlm/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 09:53:32,392 >> Model weights saved in ./finetune-mlm/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 09:53:32,393 >> tokenizer config file saved in ./finetune-mlm/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 09:53:32,393 >> Special tokens file saved in ./finetune-mlm/checkpoint-4000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 09:53:35,911 >> Deleting older checkpoint [finetune-mlm/checkpoint-3000] due to args.save_total_limit\n",
            "{'loss': 2.4624, 'learning_rate': 4.0492288189309105e-05, 'epoch': 0.95}\n",
            " 19% 4500/23665 [08:22<31:54, 10.01it/s][INFO|trainer.py:2458] 2022-06-10 09:54:27,642 >> Saving model checkpoint to ./finetune-mlm/checkpoint-4500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 09:54:27,643 >> Configuration saved in ./finetune-mlm/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 09:54:28,826 >> Model weights saved in ./finetune-mlm/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 09:54:28,827 >> tokenizer config file saved in ./finetune-mlm/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 09:54:28,827 >> Special tokens file saved in ./finetune-mlm/checkpoint-4500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 09:54:32,315 >> Deleting older checkpoint [finetune-mlm/checkpoint-3500] due to args.save_total_limit\n",
            "{'loss': 2.3842, 'learning_rate': 3.943587576589901e-05, 'epoch': 1.06}\n",
            " 21% 5000/23665 [09:19<31:50,  9.77it/s][INFO|trainer.py:2458] 2022-06-10 09:55:24,090 >> Saving model checkpoint to ./finetune-mlm/checkpoint-5000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 09:55:24,091 >> Configuration saved in ./finetune-mlm/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 09:55:25,262 >> Model weights saved in ./finetune-mlm/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 09:55:25,263 >> tokenizer config file saved in ./finetune-mlm/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 09:55:25,263 >> Special tokens file saved in ./finetune-mlm/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 09:55:28,770 >> Deleting older checkpoint [finetune-mlm/checkpoint-4000] due to args.save_total_limit\n",
            "{'loss': 2.3051, 'learning_rate': 3.837946334248891e-05, 'epoch': 1.16}\n",
            " 23% 5500/23665 [10:16<31:44,  9.54it/s][INFO|trainer.py:2458] 2022-06-10 09:56:21,081 >> Saving model checkpoint to ./finetune-mlm/checkpoint-5500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 09:56:21,082 >> Configuration saved in ./finetune-mlm/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 09:56:22,355 >> Model weights saved in ./finetune-mlm/checkpoint-5500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 09:56:22,356 >> tokenizer config file saved in ./finetune-mlm/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 09:56:22,356 >> Special tokens file saved in ./finetune-mlm/checkpoint-5500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 09:56:25,731 >> Deleting older checkpoint [finetune-mlm/checkpoint-4500] due to args.save_total_limit\n",
            "{'loss': 2.2418, 'learning_rate': 3.732305091907881e-05, 'epoch': 1.27}\n",
            " 25% 6000/23665 [11:12<29:06, 10.12it/s][INFO|trainer.py:2458] 2022-06-10 09:57:16,874 >> Saving model checkpoint to ./finetune-mlm/checkpoint-6000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 09:57:16,875 >> Configuration saved in ./finetune-mlm/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 09:57:18,208 >> Model weights saved in ./finetune-mlm/checkpoint-6000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 09:57:18,209 >> tokenizer config file saved in ./finetune-mlm/checkpoint-6000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 09:57:18,209 >> Special tokens file saved in ./finetune-mlm/checkpoint-6000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 09:57:21,574 >> Deleting older checkpoint [finetune-mlm/checkpoint-5000] due to args.save_total_limit\n",
            "{'loss': 2.2927, 'learning_rate': 3.626663849566871e-05, 'epoch': 1.37}\n",
            " 27% 6500/23665 [12:08<29:04,  9.84it/s][INFO|trainer.py:2458] 2022-06-10 09:58:13,012 >> Saving model checkpoint to ./finetune-mlm/checkpoint-6500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 09:58:13,013 >> Configuration saved in ./finetune-mlm/checkpoint-6500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 09:58:14,152 >> Model weights saved in ./finetune-mlm/checkpoint-6500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 09:58:14,153 >> tokenizer config file saved in ./finetune-mlm/checkpoint-6500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 09:58:14,153 >> Special tokens file saved in ./finetune-mlm/checkpoint-6500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 09:58:17,870 >> Deleting older checkpoint [finetune-mlm/checkpoint-5500] due to args.save_total_limit\n",
            "{'loss': 2.2988, 'learning_rate': 3.5210226072258614e-05, 'epoch': 1.48}\n",
            " 30% 7000/23665 [13:04<29:08,  9.53it/s][INFO|trainer.py:2458] 2022-06-10 09:59:09,724 >> Saving model checkpoint to ./finetune-mlm/checkpoint-7000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 09:59:09,725 >> Configuration saved in ./finetune-mlm/checkpoint-7000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 09:59:10,843 >> Model weights saved in ./finetune-mlm/checkpoint-7000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 09:59:10,843 >> tokenizer config file saved in ./finetune-mlm/checkpoint-7000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 09:59:10,844 >> Special tokens file saved in ./finetune-mlm/checkpoint-7000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 09:59:14,350 >> Deleting older checkpoint [finetune-mlm/checkpoint-6000] due to args.save_total_limit\n",
            "{'loss': 2.2227, 'learning_rate': 3.415381364884851e-05, 'epoch': 1.58}\n",
            " 32% 7500/23665 [14:01<27:30,  9.80it/s][INFO|trainer.py:2458] 2022-06-10 10:00:06,257 >> Saving model checkpoint to ./finetune-mlm/checkpoint-7500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:00:06,258 >> Configuration saved in ./finetune-mlm/checkpoint-7500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:00:07,398 >> Model weights saved in ./finetune-mlm/checkpoint-7500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:00:07,398 >> tokenizer config file saved in ./finetune-mlm/checkpoint-7500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:00:07,399 >> Special tokens file saved in ./finetune-mlm/checkpoint-7500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:00:10,893 >> Deleting older checkpoint [finetune-mlm/checkpoint-6500] due to args.save_total_limit\n",
            "{'loss': 2.3389, 'learning_rate': 3.309740122543841e-05, 'epoch': 1.69}\n",
            " 34% 8000/23665 [14:57<26:14,  9.95it/s][INFO|trainer.py:2458] 2022-06-10 10:01:02,579 >> Saving model checkpoint to ./finetune-mlm/checkpoint-8000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:01:02,580 >> Configuration saved in ./finetune-mlm/checkpoint-8000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:01:03,742 >> Model weights saved in ./finetune-mlm/checkpoint-8000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:01:03,742 >> tokenizer config file saved in ./finetune-mlm/checkpoint-8000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:01:03,743 >> Special tokens file saved in ./finetune-mlm/checkpoint-8000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:01:07,233 >> Deleting older checkpoint [finetune-mlm/checkpoint-7000] due to args.save_total_limit\n",
            "{'loss': 2.297, 'learning_rate': 3.204098880202831e-05, 'epoch': 1.8}\n",
            " 36% 8500/23665 [15:53<26:41,  9.47it/s][INFO|trainer.py:2458] 2022-06-10 10:01:58,684 >> Saving model checkpoint to ./finetune-mlm/checkpoint-8500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:01:58,685 >> Configuration saved in ./finetune-mlm/checkpoint-8500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:01:59,816 >> Model weights saved in ./finetune-mlm/checkpoint-8500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:01:59,817 >> tokenizer config file saved in ./finetune-mlm/checkpoint-8500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:01:59,817 >> Special tokens file saved in ./finetune-mlm/checkpoint-8500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:02:03,367 >> Deleting older checkpoint [finetune-mlm/checkpoint-7500] due to args.save_total_limit\n",
            "{'loss': 2.2356, 'learning_rate': 3.0984576378618214e-05, 'epoch': 1.9}\n",
            " 38% 9000/23665 [16:50<24:47,  9.86it/s][INFO|trainer.py:2458] 2022-06-10 10:02:55,140 >> Saving model checkpoint to ./finetune-mlm/checkpoint-9000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:02:55,141 >> Configuration saved in ./finetune-mlm/checkpoint-9000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:02:56,307 >> Model weights saved in ./finetune-mlm/checkpoint-9000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:02:56,308 >> tokenizer config file saved in ./finetune-mlm/checkpoint-9000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:02:56,308 >> Special tokens file saved in ./finetune-mlm/checkpoint-9000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:02:59,878 >> Deleting older checkpoint [finetune-mlm/checkpoint-8000] due to args.save_total_limit\n",
            "{'loss': 2.3024, 'learning_rate': 2.9928163955208116e-05, 'epoch': 2.01}\n",
            " 40% 9500/23665 [17:46<24:23,  9.68it/s][INFO|trainer.py:2458] 2022-06-10 10:03:51,385 >> Saving model checkpoint to ./finetune-mlm/checkpoint-9500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:03:51,386 >> Configuration saved in ./finetune-mlm/checkpoint-9500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:03:52,461 >> Model weights saved in ./finetune-mlm/checkpoint-9500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:03:52,461 >> tokenizer config file saved in ./finetune-mlm/checkpoint-9500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:03:52,462 >> Special tokens file saved in ./finetune-mlm/checkpoint-9500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:03:56,072 >> Deleting older checkpoint [finetune-mlm/checkpoint-8500] due to args.save_total_limit\n",
            "{'loss': 2.2272, 'learning_rate': 2.8871751531798014e-05, 'epoch': 2.11}\n",
            " 42% 10000/23665 [18:42<23:48,  9.57it/s][INFO|trainer.py:2458] 2022-06-10 10:04:47,612 >> Saving model checkpoint to ./finetune-mlm/checkpoint-10000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:04:47,613 >> Configuration saved in ./finetune-mlm/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:04:48,771 >> Model weights saved in ./finetune-mlm/checkpoint-10000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:04:48,772 >> tokenizer config file saved in ./finetune-mlm/checkpoint-10000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:04:48,772 >> Special tokens file saved in ./finetune-mlm/checkpoint-10000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:04:52,369 >> Deleting older checkpoint [finetune-mlm/checkpoint-9000] due to args.save_total_limit\n",
            "{'loss': 2.1666, 'learning_rate': 2.7815339108387916e-05, 'epoch': 2.22}\n",
            " 44% 10500/23665 [19:38<22:15,  9.86it/s][INFO|trainer.py:2458] 2022-06-10 10:05:43,618 >> Saving model checkpoint to ./finetune-mlm/checkpoint-10500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:05:43,620 >> Configuration saved in ./finetune-mlm/checkpoint-10500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:05:44,737 >> Model weights saved in ./finetune-mlm/checkpoint-10500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:05:44,738 >> tokenizer config file saved in ./finetune-mlm/checkpoint-10500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:05:44,738 >> Special tokens file saved in ./finetune-mlm/checkpoint-10500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:05:48,280 >> Deleting older checkpoint [finetune-mlm/checkpoint-9500] due to args.save_total_limit\n",
            "{'loss': 2.1927, 'learning_rate': 2.6758926684977814e-05, 'epoch': 2.32}\n",
            " 46% 11000/23665 [20:34<21:22,  9.88it/s][INFO|trainer.py:2458] 2022-06-10 10:06:39,824 >> Saving model checkpoint to ./finetune-mlm/checkpoint-11000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:06:39,825 >> Configuration saved in ./finetune-mlm/checkpoint-11000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:06:41,043 >> Model weights saved in ./finetune-mlm/checkpoint-11000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:06:41,043 >> tokenizer config file saved in ./finetune-mlm/checkpoint-11000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:06:41,044 >> Special tokens file saved in ./finetune-mlm/checkpoint-11000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:06:44,642 >> Deleting older checkpoint [finetune-mlm/checkpoint-10000] due to args.save_total_limit\n",
            "{'loss': 2.1275, 'learning_rate': 2.5702514261567716e-05, 'epoch': 2.43}\n",
            " 49% 11500/23665 [21:31<20:13, 10.02it/s][INFO|trainer.py:2458] 2022-06-10 10:07:36,583 >> Saving model checkpoint to ./finetune-mlm/checkpoint-11500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:07:36,584 >> Configuration saved in ./finetune-mlm/checkpoint-11500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:07:37,831 >> Model weights saved in ./finetune-mlm/checkpoint-11500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:07:37,832 >> tokenizer config file saved in ./finetune-mlm/checkpoint-11500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:07:37,832 >> Special tokens file saved in ./finetune-mlm/checkpoint-11500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:07:41,259 >> Deleting older checkpoint [finetune-mlm/checkpoint-10500] due to args.save_total_limit\n",
            "{'loss': 2.1484, 'learning_rate': 2.4646101838157618e-05, 'epoch': 2.54}\n",
            " 51% 12000/23665 [22:28<20:09,  9.64it/s][INFO|trainer.py:2458] 2022-06-10 10:08:33,116 >> Saving model checkpoint to ./finetune-mlm/checkpoint-12000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:08:33,116 >> Configuration saved in ./finetune-mlm/checkpoint-12000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:08:34,283 >> Model weights saved in ./finetune-mlm/checkpoint-12000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:08:34,284 >> tokenizer config file saved in ./finetune-mlm/checkpoint-12000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:08:34,284 >> Special tokens file saved in ./finetune-mlm/checkpoint-12000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:08:37,793 >> Deleting older checkpoint [finetune-mlm/checkpoint-11000] due to args.save_total_limit\n",
            "{'loss': 2.1411, 'learning_rate': 2.3589689414747516e-05, 'epoch': 2.64}\n",
            " 53% 12500/23665 [23:24<19:53,  9.35it/s][INFO|trainer.py:2458] 2022-06-10 10:09:29,539 >> Saving model checkpoint to ./finetune-mlm/checkpoint-12500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:09:29,540 >> Configuration saved in ./finetune-mlm/checkpoint-12500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:09:30,679 >> Model weights saved in ./finetune-mlm/checkpoint-12500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:09:30,679 >> tokenizer config file saved in ./finetune-mlm/checkpoint-12500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:09:30,680 >> Special tokens file saved in ./finetune-mlm/checkpoint-12500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:09:34,209 >> Deleting older checkpoint [finetune-mlm/checkpoint-11500] due to args.save_total_limit\n",
            "{'loss': 2.1272, 'learning_rate': 2.2533276991337418e-05, 'epoch': 2.75}\n",
            " 55% 13000/23665 [24:21<18:19,  9.70it/s][INFO|trainer.py:2458] 2022-06-10 10:10:25,863 >> Saving model checkpoint to ./finetune-mlm/checkpoint-13000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:10:25,864 >> Configuration saved in ./finetune-mlm/checkpoint-13000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:10:26,988 >> Model weights saved in ./finetune-mlm/checkpoint-13000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:10:26,989 >> tokenizer config file saved in ./finetune-mlm/checkpoint-13000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:10:26,989 >> Special tokens file saved in ./finetune-mlm/checkpoint-13000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:10:30,638 >> Deleting older checkpoint [finetune-mlm/checkpoint-12000] due to args.save_total_limit\n",
            "{'loss': 2.1321, 'learning_rate': 2.147686456792732e-05, 'epoch': 2.85}\n",
            " 57% 13500/23665 [25:17<18:10,  9.32it/s][INFO|trainer.py:2458] 2022-06-10 10:11:22,249 >> Saving model checkpoint to ./finetune-mlm/checkpoint-13500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:11:22,250 >> Configuration saved in ./finetune-mlm/checkpoint-13500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:11:23,379 >> Model weights saved in ./finetune-mlm/checkpoint-13500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:11:23,380 >> tokenizer config file saved in ./finetune-mlm/checkpoint-13500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:11:23,380 >> Special tokens file saved in ./finetune-mlm/checkpoint-13500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:11:26,948 >> Deleting older checkpoint [finetune-mlm/checkpoint-12500] due to args.save_total_limit\n",
            "{'loss': 2.0623, 'learning_rate': 2.042045214451722e-05, 'epoch': 2.96}\n",
            " 59% 14000/23665 [26:14<16:58,  9.49it/s][INFO|trainer.py:2458] 2022-06-10 10:12:18,844 >> Saving model checkpoint to ./finetune-mlm/checkpoint-14000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:12:18,845 >> Configuration saved in ./finetune-mlm/checkpoint-14000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:12:19,971 >> Model weights saved in ./finetune-mlm/checkpoint-14000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:12:19,971 >> tokenizer config file saved in ./finetune-mlm/checkpoint-14000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:12:19,971 >> Special tokens file saved in ./finetune-mlm/checkpoint-14000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:12:23,503 >> Deleting older checkpoint [finetune-mlm/checkpoint-13000] due to args.save_total_limit\n",
            "{'loss': 2.0875, 'learning_rate': 1.9364039721107123e-05, 'epoch': 3.06}\n",
            " 61% 14500/23665 [27:10<15:30,  9.85it/s][INFO|trainer.py:2458] 2022-06-10 10:13:15,189 >> Saving model checkpoint to ./finetune-mlm/checkpoint-14500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:13:15,190 >> Configuration saved in ./finetune-mlm/checkpoint-14500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:13:16,253 >> Model weights saved in ./finetune-mlm/checkpoint-14500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:13:16,253 >> tokenizer config file saved in ./finetune-mlm/checkpoint-14500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:13:16,254 >> Special tokens file saved in ./finetune-mlm/checkpoint-14500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:13:19,794 >> Deleting older checkpoint [finetune-mlm/checkpoint-13500] due to args.save_total_limit\n",
            "{'loss': 2.024, 'learning_rate': 1.8307627297697022e-05, 'epoch': 3.17}\n",
            " 63% 15000/23665 [28:06<15:16,  9.46it/s][INFO|trainer.py:2458] 2022-06-10 10:14:11,419 >> Saving model checkpoint to ./finetune-mlm/checkpoint-15000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:14:11,420 >> Configuration saved in ./finetune-mlm/checkpoint-15000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:14:12,580 >> Model weights saved in ./finetune-mlm/checkpoint-15000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:14:12,580 >> tokenizer config file saved in ./finetune-mlm/checkpoint-15000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:14:12,580 >> Special tokens file saved in ./finetune-mlm/checkpoint-15000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:14:16,109 >> Deleting older checkpoint [finetune-mlm/checkpoint-14000] due to args.save_total_limit\n",
            "{'loss': 2.0132, 'learning_rate': 1.7251214874286924e-05, 'epoch': 3.27}\n",
            " 65% 15500/23665 [29:03<13:36, 10.00it/s][INFO|trainer.py:2458] 2022-06-10 10:15:07,868 >> Saving model checkpoint to ./finetune-mlm/checkpoint-15500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:15:07,869 >> Configuration saved in ./finetune-mlm/checkpoint-15500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:15:09,011 >> Model weights saved in ./finetune-mlm/checkpoint-15500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:15:09,012 >> tokenizer config file saved in ./finetune-mlm/checkpoint-15500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:15:09,012 >> Special tokens file saved in ./finetune-mlm/checkpoint-15500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:15:12,539 >> Deleting older checkpoint [finetune-mlm/checkpoint-14500] due to args.save_total_limit\n",
            "{'loss': 1.992, 'learning_rate': 1.6194802450876822e-05, 'epoch': 3.38}\n",
            " 68% 16000/23665 [29:59<13:16,  9.62it/s][INFO|trainer.py:2458] 2022-06-10 10:16:04,324 >> Saving model checkpoint to ./finetune-mlm/checkpoint-16000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:16:04,325 >> Configuration saved in ./finetune-mlm/checkpoint-16000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:16:05,488 >> Model weights saved in ./finetune-mlm/checkpoint-16000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:16:05,488 >> tokenizer config file saved in ./finetune-mlm/checkpoint-16000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:16:05,488 >> Special tokens file saved in ./finetune-mlm/checkpoint-16000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:16:09,022 >> Deleting older checkpoint [finetune-mlm/checkpoint-15000] due to args.save_total_limit\n",
            "{'loss': 2.0004, 'learning_rate': 1.5138390027466722e-05, 'epoch': 3.49}\n",
            " 70% 16500/23665 [30:55<12:35,  9.48it/s][INFO|trainer.py:2458] 2022-06-10 10:17:00,755 >> Saving model checkpoint to ./finetune-mlm/checkpoint-16500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:17:00,755 >> Configuration saved in ./finetune-mlm/checkpoint-16500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:17:01,889 >> Model weights saved in ./finetune-mlm/checkpoint-16500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:17:01,890 >> tokenizer config file saved in ./finetune-mlm/checkpoint-16500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:17:01,890 >> Special tokens file saved in ./finetune-mlm/checkpoint-16500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:17:05,440 >> Deleting older checkpoint [finetune-mlm/checkpoint-15500] due to args.save_total_limit\n",
            "{'loss': 1.9505, 'learning_rate': 1.4081977604056624e-05, 'epoch': 3.59}\n",
            " 72% 17000/23665 [31:52<11:30,  9.66it/s][INFO|trainer.py:2458] 2022-06-10 10:17:57,546 >> Saving model checkpoint to ./finetune-mlm/checkpoint-17000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:17:57,547 >> Configuration saved in ./finetune-mlm/checkpoint-17000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:17:58,693 >> Model weights saved in ./finetune-mlm/checkpoint-17000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:17:58,694 >> tokenizer config file saved in ./finetune-mlm/checkpoint-17000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:17:58,694 >> Special tokens file saved in ./finetune-mlm/checkpoint-17000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:18:02,206 >> Deleting older checkpoint [finetune-mlm/checkpoint-16000] due to args.save_total_limit\n",
            "{'loss': 1.952, 'learning_rate': 1.3025565180646526e-05, 'epoch': 3.7}\n",
            " 74% 17500/23665 [32:48<10:27,  9.82it/s][INFO|trainer.py:2458] 2022-06-10 10:18:53,801 >> Saving model checkpoint to ./finetune-mlm/checkpoint-17500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:18:53,801 >> Configuration saved in ./finetune-mlm/checkpoint-17500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:18:54,919 >> Model weights saved in ./finetune-mlm/checkpoint-17500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:18:54,920 >> tokenizer config file saved in ./finetune-mlm/checkpoint-17500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:18:54,920 >> Special tokens file saved in ./finetune-mlm/checkpoint-17500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:18:58,410 >> Deleting older checkpoint [finetune-mlm/checkpoint-16500] due to args.save_total_limit\n",
            "{'loss': 1.9187, 'learning_rate': 1.1969152757236426e-05, 'epoch': 3.8}\n",
            " 76% 18000/23665 [33:45<09:36,  9.82it/s][INFO|trainer.py:2458] 2022-06-10 10:19:50,187 >> Saving model checkpoint to ./finetune-mlm/checkpoint-18000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:19:50,188 >> Configuration saved in ./finetune-mlm/checkpoint-18000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:19:51,311 >> Model weights saved in ./finetune-mlm/checkpoint-18000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:19:51,312 >> tokenizer config file saved in ./finetune-mlm/checkpoint-18000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:19:51,312 >> Special tokens file saved in ./finetune-mlm/checkpoint-18000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:19:54,817 >> Deleting older checkpoint [finetune-mlm/checkpoint-17000] due to args.save_total_limit\n",
            "{'loss': 1.9541, 'learning_rate': 1.0912740333826327e-05, 'epoch': 3.91}\n",
            " 78% 18500/23665 [34:41<09:02,  9.51it/s][INFO|trainer.py:2458] 2022-06-10 10:20:46,555 >> Saving model checkpoint to ./finetune-mlm/checkpoint-18500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:20:46,557 >> Configuration saved in ./finetune-mlm/checkpoint-18500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:20:47,721 >> Model weights saved in ./finetune-mlm/checkpoint-18500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:20:47,721 >> tokenizer config file saved in ./finetune-mlm/checkpoint-18500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:20:47,721 >> Special tokens file saved in ./finetune-mlm/checkpoint-18500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:20:51,270 >> Deleting older checkpoint [finetune-mlm/checkpoint-17500] due to args.save_total_limit\n",
            "{'loss': 1.9896, 'learning_rate': 9.856327910416228e-06, 'epoch': 4.01}\n",
            " 80% 19000/23665 [35:37<08:10,  9.51it/s][INFO|trainer.py:2458] 2022-06-10 10:21:42,780 >> Saving model checkpoint to ./finetune-mlm/checkpoint-19000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:21:42,781 >> Configuration saved in ./finetune-mlm/checkpoint-19000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:21:43,896 >> Model weights saved in ./finetune-mlm/checkpoint-19000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:21:43,896 >> tokenizer config file saved in ./finetune-mlm/checkpoint-19000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:21:43,896 >> Special tokens file saved in ./finetune-mlm/checkpoint-19000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:21:47,419 >> Deleting older checkpoint [finetune-mlm/checkpoint-18000] due to args.save_total_limit\n",
            "{'loss': 1.9349, 'learning_rate': 8.799915487006128e-06, 'epoch': 4.12}\n",
            " 82% 19500/23665 [36:34<07:40,  9.04it/s][INFO|trainer.py:2458] 2022-06-10 10:22:39,017 >> Saving model checkpoint to ./finetune-mlm/checkpoint-19500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:22:39,019 >> Configuration saved in ./finetune-mlm/checkpoint-19500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:22:40,234 >> Model weights saved in ./finetune-mlm/checkpoint-19500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:22:40,235 >> tokenizer config file saved in ./finetune-mlm/checkpoint-19500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:22:40,236 >> Special tokens file saved in ./finetune-mlm/checkpoint-19500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:22:43,759 >> Deleting older checkpoint [finetune-mlm/checkpoint-18500] due to args.save_total_limit\n",
            "{'loss': 1.8805, 'learning_rate': 7.743503063596028e-06, 'epoch': 4.23}\n",
            " 85% 20000/23665 [37:30<06:42,  9.12it/s][INFO|trainer.py:2458] 2022-06-10 10:23:35,474 >> Saving model checkpoint to ./finetune-mlm/checkpoint-20000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:23:35,475 >> Configuration saved in ./finetune-mlm/checkpoint-20000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:23:36,618 >> Model weights saved in ./finetune-mlm/checkpoint-20000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:23:36,618 >> tokenizer config file saved in ./finetune-mlm/checkpoint-20000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:23:36,618 >> Special tokens file saved in ./finetune-mlm/checkpoint-20000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:23:40,167 >> Deleting older checkpoint [finetune-mlm/checkpoint-19000] due to args.save_total_limit\n",
            "{'loss': 1.9161, 'learning_rate': 6.6870906401859295e-06, 'epoch': 4.33}\n",
            " 87% 20500/23665 [38:27<05:26,  9.69it/s][INFO|trainer.py:2458] 2022-06-10 10:24:32,033 >> Saving model checkpoint to ./finetune-mlm/checkpoint-20500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:24:32,033 >> Configuration saved in ./finetune-mlm/checkpoint-20500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:24:33,143 >> Model weights saved in ./finetune-mlm/checkpoint-20500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:24:33,144 >> tokenizer config file saved in ./finetune-mlm/checkpoint-20500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:24:33,144 >> Special tokens file saved in ./finetune-mlm/checkpoint-20500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:24:36,680 >> Deleting older checkpoint [finetune-mlm/checkpoint-19500] due to args.save_total_limit\n",
            "{'loss': 1.8974, 'learning_rate': 5.6306782167758296e-06, 'epoch': 4.44}\n",
            " 89% 21000/23665 [39:23<04:22, 10.16it/s][INFO|trainer.py:2458] 2022-06-10 10:25:28,316 >> Saving model checkpoint to ./finetune-mlm/checkpoint-21000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:25:28,317 >> Configuration saved in ./finetune-mlm/checkpoint-21000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:25:29,670 >> Model weights saved in ./finetune-mlm/checkpoint-21000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:25:29,670 >> tokenizer config file saved in ./finetune-mlm/checkpoint-21000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:25:29,671 >> Special tokens file saved in ./finetune-mlm/checkpoint-21000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:25:32,970 >> Deleting older checkpoint [finetune-mlm/checkpoint-20000] due to args.save_total_limit\n",
            "{'loss': 1.8864, 'learning_rate': 4.57426579336573e-06, 'epoch': 4.54}\n",
            " 91% 21500/23665 [40:19<03:37,  9.96it/s][INFO|trainer.py:2458] 2022-06-10 10:26:24,276 >> Saving model checkpoint to ./finetune-mlm/checkpoint-21500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:26:24,278 >> Configuration saved in ./finetune-mlm/checkpoint-21500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:26:25,432 >> Model weights saved in ./finetune-mlm/checkpoint-21500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:26:25,433 >> tokenizer config file saved in ./finetune-mlm/checkpoint-21500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:26:25,433 >> Special tokens file saved in ./finetune-mlm/checkpoint-21500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:26:29,258 >> Deleting older checkpoint [finetune-mlm/checkpoint-20500] due to args.save_total_limit\n",
            "{'loss': 1.9184, 'learning_rate': 3.517853369955631e-06, 'epoch': 4.65}\n",
            " 93% 22000/23665 [41:15<02:53,  9.61it/s][INFO|trainer.py:2458] 2022-06-10 10:27:20,662 >> Saving model checkpoint to ./finetune-mlm/checkpoint-22000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:27:20,663 >> Configuration saved in ./finetune-mlm/checkpoint-22000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:27:21,838 >> Model weights saved in ./finetune-mlm/checkpoint-22000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:27:21,839 >> tokenizer config file saved in ./finetune-mlm/checkpoint-22000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:27:21,839 >> Special tokens file saved in ./finetune-mlm/checkpoint-22000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:27:25,472 >> Deleting older checkpoint [finetune-mlm/checkpoint-21000] due to args.save_total_limit\n",
            "{'loss': 1.8813, 'learning_rate': 2.4614409465455315e-06, 'epoch': 4.75}\n",
            " 95% 22500/23665 [42:12<01:59,  9.78it/s][INFO|trainer.py:2458] 2022-06-10 10:28:17,582 >> Saving model checkpoint to ./finetune-mlm/checkpoint-22500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:28:17,583 >> Configuration saved in ./finetune-mlm/checkpoint-22500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:28:18,752 >> Model weights saved in ./finetune-mlm/checkpoint-22500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:28:18,752 >> tokenizer config file saved in ./finetune-mlm/checkpoint-22500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:28:18,753 >> Special tokens file saved in ./finetune-mlm/checkpoint-22500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:28:22,355 >> Deleting older checkpoint [finetune-mlm/checkpoint-21500] due to args.save_total_limit\n",
            "{'loss': 1.8858, 'learning_rate': 1.405028523135432e-06, 'epoch': 4.86}\n",
            " 97% 23000/23665 [43:09<01:11,  9.36it/s][INFO|trainer.py:2458] 2022-06-10 10:29:13,866 >> Saving model checkpoint to ./finetune-mlm/checkpoint-23000\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:29:13,867 >> Configuration saved in ./finetune-mlm/checkpoint-23000/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:29:15,038 >> Model weights saved in ./finetune-mlm/checkpoint-23000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:29:15,039 >> tokenizer config file saved in ./finetune-mlm/checkpoint-23000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:29:15,039 >> Special tokens file saved in ./finetune-mlm/checkpoint-23000/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:29:18,622 >> Deleting older checkpoint [finetune-mlm/checkpoint-22000] due to args.save_total_limit\n",
            "{'loss': 1.9388, 'learning_rate': 3.486160997253328e-07, 'epoch': 4.97}\n",
            " 99% 23500/23665 [44:05<00:17,  9.50it/s][INFO|trainer.py:2458] 2022-06-10 10:30:10,110 >> Saving model checkpoint to ./finetune-mlm/checkpoint-23500\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:30:10,111 >> Configuration saved in ./finetune-mlm/checkpoint-23500/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:30:11,270 >> Model weights saved in ./finetune-mlm/checkpoint-23500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:30:11,270 >> tokenizer config file saved in ./finetune-mlm/checkpoint-23500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:30:11,270 >> Special tokens file saved in ./finetune-mlm/checkpoint-23500/special_tokens_map.json\n",
            "[INFO|trainer.py:2536] 2022-06-10 10:30:14,837 >> Deleting older checkpoint [finetune-mlm/checkpoint-22500] due to args.save_total_limit\n",
            "100% 23665/23665 [44:27<00:00, 10.15it/s][INFO|trainer.py:1717] 2022-06-10 10:30:32,069 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2667.247, 'train_samples_per_second': 141.946, 'train_steps_per_second': 8.872, 'train_loss': 2.1713065554620647, 'epoch': 5.0}\n",
            "100% 23665/23665 [44:27<00:00,  8.87it/s]\n",
            "[INFO|trainer.py:2458] 2022-06-10 10:30:32,074 >> Saving model checkpoint to ./finetune-mlm\n",
            "[INFO|configuration_utils.py:446] 2022-06-10 10:30:32,075 >> Configuration saved in ./finetune-mlm/config.json\n",
            "[INFO|modeling_utils.py:1657] 2022-06-10 10:30:33,236 >> Model weights saved in ./finetune-mlm/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-06-10 10:30:33,237 >> tokenizer config file saved in ./finetune-mlm/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-06-10 10:30:33,237 >> Special tokens file saved in ./finetune-mlm/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     2.1713\n",
            "  train_runtime            = 0:44:27.24\n",
            "  train_samples            =      75721\n",
            "  train_samples_per_second =    141.946\n",
            "  train_steps_per_second   =      8.872\n",
            "06/10/2022 10:30:33 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:649] 2022-06-10 10:30:33,318 >> The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2708] 2022-06-10 10:30:33,567 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2710] 2022-06-10 10:30:33,567 >>   Num examples = 10814\n",
            "[INFO|trainer.py:2713] 2022-06-10 10:30:33,567 >>   Batch size = 16\n",
            "100% 673/676 [00:15<00:00, 42.75it/s]06/10/2022 10:30:49 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
            "100% 676/676 [00:16<00:00, 41.86it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.6297\n",
            "  eval_loss               =     2.0755\n",
            "  eval_runtime            = 0:00:16.24\n",
            "  eval_samples            =      10814\n",
            "  eval_samples_per_second =    665.532\n",
            "  eval_steps_per_second   =     41.603\n",
            "  perplexity              =     7.9688\n",
            "[INFO|modelcard.py:460] 2022-06-10 10:30:50,761 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Masked Language Modeling', 'type': 'fill-mask'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6296977124183006}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r ./finetune-mlm /content/drive/MyDrive/QA/finetune/sq/"
      ],
      "metadata": {
        "id": "ImJkovohknvu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8K2Pn_2U67j-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}